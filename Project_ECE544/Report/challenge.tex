\section{Challenges} \label{challenges}

\subsection{Radar Imaging Primer}
Radar generates images of objects by localizing point reflector models corresponds to the object. These point reflector models in the 3D space lie in voxels in a spherical coordinate determined by distance, azimuth and elevation angles. Distance is measured as the round-trip ToF (Time-of-Flight) of reflected radar signal times the speed of light. The azimuth and elevation angles can be either regarded as the beam steering angle of phased array antennas, or be estimated with DoA (Direction-of-Arrival) estimation algorithms such as beam-forming or MUSIC (Multiple Signal Classification). Notice that at further distance, the voxel size increases, so that long-range objects spread across much fewer voxels so that there shapes become more blurry. Besides, unlike the extremely narrow light beams, the cone-shape radar beams with sidelobes cause interference and leakage among voxels, which furthermore smears radar images and introduces noise. Last but not least, radar reflection tends to be more specular than the mostly scattering at optical frequencies. To sum up, "edge detection" in radar images is very different from that of computer vision. Firstly, it needs to predict the high frequency information with only the low frequency data. Secondly, it needs to fill in the missing parts of objects. 

% within a voxel of a couple centimeter deep by a couple degree wide, there is actually a continuum of a large number of point reflectors. They sum up and makes it an under-determined problem to localize them.

\subsection{Related Work}
Previous works have attempted to adapt the optical camera-oriented Convolutional Neural Network (CNN) to its microwave counterpart to classify a single object in high-resolution 2D SAR (Synthetic Aperture Radar) image ~\cite{SAR_DL, ship_SAR, change_SAR}. There has also been successful application of CNNs on recreating  short range human body skeletons in 2D planes and 3D spaces from radar images by tracking 14 key points ~\cite{rfpose, rfpose3D}. However, the scope of these applications of neural networks are very restricted features and single object classification. Besides, their radar image are either of super high resolution generated by airborne geographical sensing synthetic aperture radars or within a short range where voxel resolution does not degrade too much. On the contrary, in our application of autonomous cars, we do not have as high resolution with way smaller antenna array at longer distance, but we need precisely traced boundaries with the information of size, shape, and orientation contained. Therefore, if we simply extend the layer and number of output neurons for fine grained feature detection to form boundaries, we might need a very deep neural network. Given CNN structures haven't be well investigated for radar signal and images, this task can be extremely difficult. However, the emerge of GAN (Generative Adversarial Networks) and conditional GAN provides us great opportunity to transfer radar images from the domain of low resolution to the domain of higher resolution with sharp boundary and missing parts and details filled up. 

\subsection{Dataset Availability}
Once we chose conditional GAN as the network structure, we came across the problem of dataset availability. Ideally, we should provide a large number of radar images of various environments that self-driving cars would experience, as well as the precisely traced object boundaries and orientations as the ground truth. However, we were not able to find any public dataset of 3D or even 2D radar images, especially street views for the self-driving car application. Also in contrast to the standard images generated by cameras and LiDARs from different manufactures, radar images generated by different system and algorithms have non-negligible variation. Thus we were initially forced to build our own dataset of 3D radar images with our custom-built 60 GHz mmWave radar imaging system. However, since our system was design to compensate the frame rate for lower cost and higher resolution, even collecting 500 frames continuously will take at least 41.67 hours. Not to mention that we also have to wisely choose plenty different locations to mimic various scenes. The time cost of generating this dataset is impractical. Besides, for the ground truth dataset we even have to perfectly sync our reference camera or LiDAR for ground truth with the radar, so that the radar images would share the same point of view as the photos and LiDAR point clouds. All of these overheads add up to a huge roadblock at the starting line of our project. Therefore, instead of building dataset from real radar images, we started to leverage the well-developed EM (Electromagnetic) simulation tools and the exquisitely measured waveform of our radar waveform to simulate radar reflections and feed the simulated radar signal to the same image processing algorithm we used to synthesize radar images. In order to make our synthesized scenes and images as similar to actual road environments as possible, we try to recover them from video recording of street view in datasets like the Cityscapes ~\cite{cityscapes}.   

%\subsection{3D Complexity}
%3.3D
%3D CNN % rf 3D pose	
%3D GAN size complexity
