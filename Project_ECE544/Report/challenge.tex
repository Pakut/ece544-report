<<<<<<< HEAD
\section{Challenges}
=======
\section{Challenges} \label{challenges}
>>>>>>> 330065aa07b6b3d5312ebd03bb3832a23dbfe392
\subsection{Radar Imaging Primer}
Radar generates images of objects by localizing the cluster of point reflectors that forms the object. 3D space imaging requires mapping point reflectors to voxels in a spherical coordinates with its distance, azimuth angle, and elevation angle. Distance is measured through to round-trip Time-of-Flight (ToF) of reflected radar signal, while azimuth and elevation angles can be either obtained by the beam steering angle of phased array antennas or be estimated with Direction-of-Arrival (DoA) estimation algorithms such as beam-forming or Multiple Signal Classification (MUSIC). Notice that as distance becomes further, the voxel size corresponds to the same angle increases, so that long-range objects contain much fewer voxels and appear to be more blurry. Besides, unlike the extremely narrow width of light beams, the cone-shape radar beam with sidelobes cause interference and leakage among nearby voxels and even the environment, which smears generated images. Last but not least, with a few centimeter resolution of distance measurement, a continuum of a large number of point reflector sums up and makes it an under-determined problem to localize them. Besides, radar reflection tends to be more specular than the mostly scattering reflection at optical frequencies. In other words, reflection off smooth objects might mainly towards an angle away from the radar receiver and disappears in the image. Hence, "edge detection" in radar images is very different from that of pictures. Firstly, it needs to predict high frequency information with only low frequency data. Secondly, it needs to learn to fill up missing parts of the object. 

\subsection{Related Work}
Previous works have attempted to adapt the optical camera-oriented Convolutional Neural Network (CNN) to its microwave counterpart to classify single-object images from high-resolution 2D synthetic aperture radar (SAR) images. ~\cite{SAR_DL} ~\cite{ship_SAR} ~\cite{change_SAR} There has also been successful application of CNNs on recreating short range human body skeletons in 2D images and 3D spaces from radar images by tracking 14 key points. ~\cite{rfpose} ~\cite{rfpose3D} However, the scope of these application of neural networks on radar images are restricted to feature and single object classification. Besides, their radar image are either of super high resolution generated with airborne geographical sensing synthetic aperture radar or short-range where voxel resolution does degrade too much. On the contrary, in our application of autonomous cars, we not only don't have as high resolution for long range, but required precisely traced boundary which contains the information of size, shape, and orientation of cars, bikes, and pedestrians. Therefore if we rely on fine grained feature detection that consists boundaries, we need very deep neural nets. Since CNN structures haven't been well investigated for radar signal and image, this task becomes extremely difficult.


\subsection{Dataset Availability}
Once the network is setup, it needs training data -i.e., it needs many labeled exp
Dataset
	Variation between systems 
	Experiment
	Processing

\subsection{3D Complexity}
3.3D
3D CNN % rf 3D pose	
3D GAN size complexity


\subsection{Evaluation Metrics}


