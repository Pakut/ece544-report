\subsection{Dataset Generation}
As analyzed in section \ref{challenges}, when exploiting the application of deep learning in the new field of radar images, the problem of dataset availability is inevitable. Between the two options of building our own dataset: collecting real radar images through experiments and processing synthesized radar reflection, we have to make a trade-off. Although synthesizing dataset might be the only feasible way to create a big enough training dataset, the compatibility of trained model to real radar images significantly depends on the closeness between synthesized  and real radar reflection. Luckily, the transmission, propagation, and reflection of radar waveform are thoroughly studied and can be precisely modeled and simulated. Furthermore, with the help of advanced electromagnetic simulation softwares such as FEKO we can even model the attenuation and specularity of various objects and surfaces. Therefore, a well-designed radar reflection synthesizing algorithm should be indistinguishable from read radar signal, so it is hopeful that the performance of a cGAN model trained with synthesized radar images should not degrade too much when we use real radar image for testing instead. We are also planning to mixing a smaller number of real radar images with the synthesized dataset for training and find the improvement of cGAN performance with respect to the portion of real and synthesized data.  

The radar image synthesizing process can be further separated into: scene generation, reflector modeling, radar signal simulation, and image processing. The last two steps are straightforward, while the challenge lies in creating a realistic scene and reflector model. There are two sources: recovering from video recording of street scenes, such as the Cityscapes dataset ~\cite{cityscapes}, or composing with 3D CAD models of typical objects like cars, pedestrians from datasets such as the Deformable 3D Cuboid Model dataset and the 3D ShapeNets dataset ~\cite{3Ddata, shapenets}. Video recording from the Cityscapes dataset depicts real road conditions form the view of a car, but it lacks distance information of objects, we won't be able to recover 3D shape and accurate location. On the other hand, with the 3D CAD models it's hard to place objects in a realistic way. In this project, we try to use both. For 2D we use vision-based object detect neural networks to detect and label objects, as well as depicting the boundaries. pros: large dataset with car truck human cons: No 3D info, no specularity. For the 3D we arbitrarily find a point of view and obtain the visible 3D shape of objects from different angle. We can also simulate specular reflection. And we are planning to combine these two datasets into a realistic 3D street scene dataset by finding the 3D model the best matches the object and orientation in the video recording and placing 3D models in the way that the picture suggests. pros: small dataset, single element 
cons: 3D shape info, specularity  


% Experiment section
2D Synthesizing 
	We run Mask R-CNN on the cityscapes dataset, mask and select cars 
	
	input: radar image
	groundtruth: mask
Figure of 2D sequence
	
3D Synthesizing 
	input: contour
	groundtruth:
	mesh, fill surfaces get reflector model, pick point of view select point reflect in sight, sparsify clusters
Figure of 3D sequence

Real 
single 60 GHz radio front-end transmitting and received FMCW waveform 1.5 GHz at 61 GHz, we move on linear stage for 5 cm by 5 cm aperture of 20 wavelengthes. resolution 10 cm and degree. 10 minutes. Maximum Likelihood or FFT + beamforming to generate radar images. Figure of setup  
		
	

