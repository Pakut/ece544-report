\subsection{Dataset Generation} \label{dataset}
As analyzed in section \ref{challenges}, when exploiting the application of deep learning in the new field of radar images, the problem of dataset availability is inevitable. Between the two options of building our own dataset: collecting real radar images through experiments and processing synthesized radar reflection, we have to make a trade-off. Although synthesizing dataset might be the only feasible way to create a big enough training dataset, the compatibility of trained model to real radar images significantly depends on the closeness between synthesized  and real radar reflection. Luckily, the transmission, propagation, and reflection of radar waveforms have been thoroughly studied so that they can be precisely modeled and simulated. Furthermore, with the help of advanced electromagnetic simulation softwares such as FEKO ~\cite{feko}, we can even model the attenuation and specularity of various objects and surfaces. Therefore, a well-designed radar reflection synthesizing algorithm should be indistinguishable from read radar signal, so it is hopeful that the performance of a conditional GAN model trained with synthesized radar images should not degrade too much when we use real radar image for testing instead. We are also planning to mixing a smaller number of real radar images with the synthesized dataset for training and find the improvement of cGAN performance with respect to the portion of real and synthesized data.  

The radar image synthesizing process can be further separated into four steps: scene generation, radar reflector modeling, radar signal simulation, and image processing. The last two steps are standard and straightforward, while the challenge lies in creating the 3D space distribution of point reflectors based on a realistic scene of automotives. We found two types of dataset that can be of great use for us. The first type of dataset is 2D video recording of street scenes from the view of a car, such as the Cityscapes dataset ~\cite{cityscapes}, while the second type of dataset is 3D CAD models of typical objects on the road. Good examples are the Deformable 3D Cuboid Model dataset and the 3D ShapeNets dataset ~\cite{3Ddata, shapenets}. These two types of datasets contains unique information and make great complement to the other.

Video recordings from the Cityscapes includes a wide range of practical urban road environment from the point of view of a car, but the 3D shape of objects cannot be recovered at all. Besides, we could only very roughly estimate the location of most common objects like vehicles and human, so that from the 2D pictures of street view, we can at most place 2D shapes at roughly estimated angle and distance. On the other hand, the 3D CAD models provide us with precise 3D distribution of point reflectors that consists typical objects on the road such as cars, trucks, cyclists, and pedestrians. We can also find out the effective reflectors in the sight from an arbitrary angle, and even the specularity of reflection off surfaces. However, it is not trivial to place models and compose a verisimilar 3D scene. In our experiments, we first tried to create scenes with 2D and 3D datasets separately. For the 2D dataset, we use vision-based object detection neural networks Mask R-CNN ~\cite{rcnn} to detect masks of objects and label them, and then roughly estimate and place 2D shapes of objects based on their size and pixel-wise position to recover scenes in 3D space. After that, we assign a number of point reflectors to every objects according to their shape and labels, and from there radar images can be synthesized. In this way we can potentially build a dataset with the same size of the Cityscapes, although the EM simulation takes time and the synthesized 3D radar images are of 2D shapes. However, we want to argue that if we project the 3D synthesized image back to onto a screen, the effect of distance estimation of objects on the output image can be ignored. We are also planning to combine the 2D and 3D datasets to compensate the shortcomings of using 2D video recordings alone, and synthesize a realistic 3D street scene dataset. We will start with finding the 3D model of the object type classified by the Mask R-CNN, together with a point of view, and even distance that best fits the object mask in the video recording. According to this more precise location and orientation estimation, we can place the 3D models at almost the same place as in the video. Followed by the effective point reflector assignment with the specularity into consideration, and radar signal and imaging simulation. We can synthesize very authentic 3D radar images only at the cost of computation time.


		
	

