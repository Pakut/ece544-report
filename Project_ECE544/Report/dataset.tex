\subsection{Dataset Generation}
As analyzed in section \ref{challenges}, when exploiting the application of deep learning in the new field of radar images, the problem of dataset availability is inevitable. Between the two options of building our own dataset: collecting real radar images through experiments and processing synthesized radar reflection, we have to make a trade-off. Although synthesizing dataset might be the only feasible way to create a big enough training dataset, the compatibility of trained model to real radar images significantly depends on the closeness between synthesized  and real radar reflection. Luckily, the transmission, propagation, and reflection of radar waveform are thoroughly studied and can be precisely modeled and simulated. Furthermore, with the help of advanced electromagnetic simulation softwares such as FEKO we can even model the attenuation and specularity of various objects and surfaces. Therefore, a well-designed radar reflection synthesizing algorithm should be indistinguishable from read radar signal, so it is hopeful that the performance of a cGAN model trained with synthesized radar images should not degrade too much when we use real radar image for testing instead. We are also planning to mixing a smaller number of real radar images with the synthesized dataset for training and find the improvement of cGAN performance with respect to the portion of real and synthesized data.  

The radar image synthesizing process can be further separated into four steps: scene generation, radar reflector modeling, radar signal simulation, and image processing. The last two steps are standard and straightforward, while the challenge lies in creating the 3D space distribution of point reflectors based on a realistic scene of automotives. We found two types of dataset that can be of great use for us. The first type of dataset is video recording of street scenes from the view of a car, such as the Cityscapes dataset ~\cite{cityscapes}, while the second type of dataset of the 3D CAD models of typical objects on the road. Good examples are the Deformable 3D Cuboid Model dataset and the 3D ShapeNets dataset ~\cite{3Ddata, shapenets}. These two types of datasets contains unique information and make great complement to the other.

Video recordings from the Cityscapes includes almost a wide range of practical urban road environment from the an autonomous car, but the 3D shapes of objects cannot be recovered at all. Besides, we could only very roughly estimate the location of most common objects like vehicles and human. On the other hand, with the 3D CAD models we can obtain the 3D shape of object distribution . In this project, we first tried to use only 2D. For 2D we use vision-based object detect neural networks to detect and label objects, as well as depicting the boundaries. pros: large dataset with car truck human cons: No 3D info, no specularity. 
For the 3D we arbitrarily find a point of view and obtain the visible 3D shape of objects from different angle. We can also simulate specular reflection. And we are planning to combine these two datasets into a realistic 3D street scene dataset by finding the 3D model the best matches the object and orientation in the video recording and placing 3D models in the way that the picture suggests. pros: small dataset, single element 
cons: 3D shape info, specularity  


% Experiment section
2D Synthesizing 
	We run Mask R-CNN on the cityscapes dataset, mask and select cars 
	
	input: radar image
	groundtruth: mask
Figure of 2D sequence
	
3D Synthesizing 
	input: contour
	groundtruth:
	mesh, fill surfaces get reflector model, pick point of view select point reflect in sight, sparsify clusters
Figure of 3D sequence

Real 
single 60 GHz radio front-end transmitting and received FMCW waveform 1.5 GHz at 61 GHz, we move on linear stage for 5 cm by 5 cm aperture of 20 wavelengthes. resolution 10 cm and degree. 10 minutes. Maximum Likelihood or FFT + beamforming to generate radar images. Figure of setup  
		
	

