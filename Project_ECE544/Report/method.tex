\section{ Method}
\textcolor{red}{
Describe the overall method on how you solve the proposed problem, and a bit of original derivation that has some relevance to what youâ€™re trying to accomplish
}

Problem statement: low blured images no boundary can be seen, specularity causes missing parts, no availbale dataset, 4D CNN NN complex.

Overview 3D, in the method, we say that we start with 2D version, and based on that build 3D. 
We are trying to use cGAN to generate higher resolution images from low resolution radar images, which should have a sharp and accurate boundary of the object. Also, the missing parts due to specualarity of reflection need to be feel up.  S

cGAN:
	Why cGAN:
	
	GAN is tranfering images from 1 domain to another, some application 
	
	Generative Adverserial Networks (or GANs) have been widely used to generate images [cite some stuff here] and fill in missing parts of data. In our case, we are looking to generate images with accurate boundaries using low resolution images with missing parts as input. Conditional GANs have already proven successful in similar settings, such as in \cite{hams}, where the authors have used thermal images under low light conditions where there some parts of the image are missing to retrieve the human face boundaries and estimate its orientation. Another motivation behind using GANs is that generic loss functions such as the $L_2$ and $L_1$ (i.e. A loss function that tries to minimize the Euclidean or $L_1$ distance between the input and the output) which are the de facto standard loss function for restoring images render blurry images, which are not suitable for our application. On the other hand, we try to motivate the loss in GAN to learn to focus on the boundaries, by designing the ground to contain information mostly about the boundary of objects, as we shall see in the dataset section
	CGANs take as input a 
	Transition: cGAN has input, groundtruth, G, D
	
	Input: Low resolution radar images, because there is no available public dataset of mmWave radar images, and collecting a big enough dataset by ourselves is not possible. We synthesized radar images. This heat-map is fed into the network. 
	Groundtruth: 
	1. Size of 3D 	which makes the training phase very slow even when using GPUs. 
	2. 2D:
	3. 3D


The input to our problem is pre-processed radar data. First, using the raw data from the antenna array, a coarse heat-map of objects are generated. After some further processing. 

Why not raw data: (Goes to detail of Dataset jayden)
Radar imaging processing algorithms is a well establish feild for 70 years. and there is fruitless to try and learn them using machine learning.
So instead of  The reason why we did not choose the raw data as input is twofold. 