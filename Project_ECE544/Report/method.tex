\section{ Method}
\textcolor{red}{
Describe the overall method on how you solve the proposed problem, and a bit of original derivation that has some relevance to what youâ€™re trying to accomplish
}

Problem statement: low blured images no boundary can be seen, specularity causes missing parts, no availbale dataset, 4D CNN NN complex.

Overview 3D, in the method, we say that we start with 2D version, and based on that build 3D. 
We are trying to use cGAN to generate higher resolution images from low resolution radar images, which should have a sharp and accurate boundary of the object. Also, the missing parts due to specualarity of reflection need to be feel up.  S

	
Generative Adverserial Networks (or GANs) have been widely used to generate images [cite some stuff here] and fill in missing parts of data. In our case, we are looking to generate images with accurate boundaries using low resolution images with missing parts as input. Conditional GANs have already proven successful in similar settings, such as in \cite{hams}, where the authors have used thermal images under low light conditions where there some parts of the image are missing to retrieve the human face boundaries and estimate its orientation. Another motivation behind using conditional GANs is that loss functions such as the $L_2$ and $L_1$ (i.e. loss functions that are equal the Euclidean or $L_1$ distance between the input and the output) which are the de facto standard loss function for restoring images render blurry images, which are not suitable for our application as they do not emphasize on boundaries. On the other hand, using conditional GAN, we were be able to motivate the loss function in GAN to learn to focus on the boundaries, by designing the ground to contain information mostly about the boundary of objects, as discussed in more detail the dataset section.
	
We have adapted the CGAN from [cite pix2pix] to train and test our model. Denoting the input, ground truth, and noise by $x$, $y$, and $z$ respectively, we can write the objective for a conditional GAN where the Generator and discriminator are $G$ and $D$ as
\begin{equation}
\begin{array}{l}
\min_G \max_D \mathcal{V}_{CGAN}(D,G) = \mathbb{E}_{x}(\log(D(x|y)))\\
\, \, \, \,+ \, \, \, \mathbb{E}_{z}(\log(1-D(x,G(x|z)))).
\end{array}
\end{equation}
Here, $D()$ is the score that the discriminator gives for a certain input, and $G(x,z)$ is what the generator tries to generate given the input $x$ and noise vector $z$. The point of having the noise vector, of course, it to avoid the problem of over-fitting. The difference between the standard GAN and the conditional version is that in the latter everything is conditioned to $y$. This $y$ could be anything we want, that adds extra information about what the output $G$ should look like. In our setting, we call $y$ the ground truth, as it contains information of real boundaries of the objects. Given $y$, $D$ tries to maximize its output value when the input is real, and at the same time minimize it when the input is artificially generated by the $G$, the generator. At the same time, the generator tries to generate an image that gets a high score from $D$, motivating it to generate images that are similar to real ones.

It is suggested in [pix2pix] that we can combine generator's task of trying to get a high score from discriminator with a pre-determined loss function, such as $L_1$. That is, one could change the objective to be
<<<<<<< HEAD
\begin{equation}
\begin{array}{l}
\min_G \max_D \mathcal{V}_{CGAN}(D,G) =\\
=======
\[
\begin{array}{l}
\min_G \max_D \mathcal{V}_{CGAN}(D,G) =
>>>>>>> 330065aa07b6b3d5312ebd03bb3832a23dbfe392
\mathbb{E}_{x}(\log(D(x|y))) \, \, \, \,+ \, \, \, \mathbb{E}_{z}(\log(1-D(x,G(x|z))))\\
+\,\lambda \, \mathcal{L}_{L_1}(G(x,z)).
\end{array}
\]
The motivation behind this is to capture the low-frequency information using the $L_1$ loss, and motivate GAN to model high-frequency information, which in our case, will translate to more precision in identifying boundaries.

For the generator, the authors of [pix2pix] used U-net [cite unet]. Similar to most architectures, U-net consists of a contracting path and an expansive path. The first layer contracting path is made of two successive $3 \times 3$ convolutions followed by a ReLu (rectified linear unit) and a $2 \times 2$ max-pooling layer without overlapping, which shrinks the size of its input by a factor of four. Let us call such a layer a down-sampling layer. This layer is then repeated multiple time, each time doubling the feature channels. The expansive path reverses this process: at each layer, the data is first up-sampled by a factor of two, 
As for the discriminator, what we need is for the network to be able to identify local structures, in order to capture the properties of local objects (e.g. cars). Since the network is relying on the $L_1$ loss to guarantee the correctness of low-frequency information, it is possible to restrict the discriminator to only penalize structures that occur within a patch window of the image. In other words, the discriminator slides over the image looking at patches of size $n \times n$, and scoring each of them, and finally averaging over all patches to derive the final score. For our implementation, we chose $n$ to be 70 where the image size was $256 \times 256$. This structure has been dubbed patchGAN [pix2pix].

Input: Low resolution radar images, because there is no available public dataset of mmWave radar images, and collecting a big enough dataset by ourselves is not possible. We synthesized radar images. This heat-map is fed into the network. 
	Groundtruth: 
	1. Size of 3D 	which makes the training phase very slow even when using GPUs. 
	2. 2D:
	3. 3D


The input to our problem is pre-processed radar data. First, using the raw data from the antenna array, a coarse heat-map of objects are generated. After some further processing. 

Why not raw data: (Goes to detail of Dataset jayden)
Radar imaging processing algorithms is a well establish feild for 70 years. and there is fruitless to try and learn them using machine learning.
So instead of  The reason why we did not choose the raw data as input is twofold. 